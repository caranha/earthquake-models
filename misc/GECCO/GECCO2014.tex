% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%

\documentclass{sig-alternate}

\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}

\conferenceinfo{GECCO'14}{July 12-16,2014, Vancouver, BC, Canada.}
\CopyrightYear{2014}
\crdata{TBA}
\clubpenalty=10000
\widowpenalty=10000

\title{Is it possible to generate good Earthquake Risk Models using Genetic Algorithms?}

\numberofauthors{4} 

\author{
\alignauthor Blind Author 1\\
       \affaddr{University 1}\\
       \affaddr{Address 1}\\
       \affaddr{Address 2}\\
       \email{e-mail@address.com}
\alignauthor Blind Author 2\\
       \affaddr{University 1}\\
       \affaddr{Address 1}\\
       \affaddr{Address 2}\\
       \email{e-mail@address.com}
\and
\alignauthor Blind Author 3\\
       \affaddr{University 1}\\
       \affaddr{Address 1}\\
       \affaddr{Address 2}\\
       \email{e-mail@address.com}
\alignauthor Blind Author 4\\
       \affaddr{University 1}\\
       \affaddr{Address 1}\\
       \affaddr{Address 2}\\
       \email{e-mail@address.com}
}

\date{29 January 2014}


\maketitle
\begin{abstract}
Understanding the mechanisms and patterns of earthquake occurrence is of
crucial importance for assessing and mitigating the seismic risk. In this
work we analyze the viability of using Evolutionary Computation (EC) as a
means of generating models for the occurrence of earthquakes. Our proposal
is made in the context of the "Collaboratory for the Study of Earthquake
Predictability" (CSEP), an international effort to standardize the study
and testing of earthquake forecasting models.

We use a standard Genetic Algorithm (GA) with real valued genome, where
each allele corresponds to a bin in the forecast model. The design of an
appropriate fitness function is the main challenge for this task, and we
test three different proposals, all based on the log-likelihood of the
candidate model against the training data set.

The resulting forecasting models are compared with statistical models
traditionally employed by the CSEP community, using data from the
Japan Meteorological Agency (JMA) earthquake catalog. Our results
indicate promise for the use of GA as basis for constructing
statistical earthquake forecast models. Based on these results, we
identify research directions that we consider deserve more attention
from the EC community.
\end{abstract}

% A category with the (minimum) three required fields
\category{I.2.8}{Artificial Intelligence}{Problem Solving, Control
  Methods and Search Heuristic Methods}
%A category including the fourth, optional field follows...
\category{J.2}{Computer Application}{Physical Sciences and
  Engineering}[Earth and Atmospheric Sciences]

\terms{Real World Applications}

\keywords{Earthquakes, Risk Models, Genetic Algorithms, Forecasting}

\section{Introduction}
%% TO REVIEW

Earthquakes pose a huge risk for human society, in their potential for
large scale loss of life and destruction of infra-structure. In the
last decade, large earthquakes such as Sumatra (2004), Kashmir (2005),
Sichuan (2008) and Tohoku (2011) caused terrible amounts of casualties.

Therefore, it is crucially important to understand the patterns and
mechanisms behind the occurrence of earthquakes. This knowledge may
allow us to create better seismic risk forecast models, indicating
where, when and how often large earthquakes are expected to
happen. Such information can be put to good use for mitigating damage
through urban planning, civil engineering codes, emergency
preparedness, et cetera.

Still, earthquake prediction is still a largely unsolved problem, and
the specific geological mechanisms behind earthquake generation are
not completely understood. Therefore this is a very active topic of
research in the seismology community. There are many proposed models,
drawing from both geophysical principles and statistical analysis of
previous seismic activity.

%% TODO: CITE HUMIES
Surprisingly enough, applications of Evolutionary Computation (EC) for
this problem have been few and far between. Given that EC has some
times managed to find better solutions than humans for many hard
problems, we wonder if Artificial Evolution might not be able to find
something that humans are missing.

Therefore, the goal of this paper is to explore the suitability of
Evolutionary Computation to the problem of earthquake forecast
generation. To this end, we outline the problem as understood today,
with a focus on the ``best practices'' suggested by the
\emph{Collaboratory for the Study of Earthquake Predictability} (CSEP)
for testing earthquake forecast models.

Using this information, we design and implement a simple Genetic
Algorithm that evolves a forecast model, the \emph{GAModel}. We
compare the GAModel with the Relative Intensity (RI) algorithm and an
information-less forecast. These comparisons use the CSEP-recommended
Area Skill Score (ASS) test, using data from the Japanese
Metereological Agency (JMA) between 2005 and 2012.

The GA-based forecasts are generally competitive when compared to the
RI algorithm, even though they still have difficulty in some of the
harder scenarios. The results shows that while there is vast room for
improvement, Evolutionary Algorithm approaches definitely have
potential to collaborate with the study of Earthquake Predictability.

%% I hate this kind of paragraph.
Section 2 of this papers outlines the earthquake prediction
problem. Section 3 reviews how Evolutionary Computation has been used
up until now in the context of seismic research. In section 4, we
detail the Genetic Algorithm system for generating forecast models. In
section 5 we compare the performance of this system with the RI
algorithm, and the implications of the results are discussed in the
last section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Earthquake Prediction Problem}
%% TO Review

In the field of seismology, there is a large number of model proposals
for earthquake forecasting. These range from methods based on
geophisical principles to purely statistical algorithms.

In this context, the Collaboratory Study for Earthquake Predictability
(CSEP)\footnote{http://www.cseptesting.org} proposes a methodology for
rigorous scientific testing of these widely varying models. The main
form of this testing is through distributed virtual
laboratories~\cite{Nanjo2011b}, to which multiple research groups
submit their forecast methods. However, the CSEP also makes its
testing suite publicly available, so that researchers can develop and
test their models.

In many application domains, the lack of an unified testing protocol
means that comparing methods developed by different research groups
can be a great burden. The CSEP framework allows us to easily and
objectively compare our proposed algorithms with other approaches.

The CSEP testing framework defines how to represent a forecast, and
defines 6 statistical tests. These will be briefly outlined below.

\subsection{Earthquake Forecast Model}\label{CSEP_definition} 
%% TO REVIEW

\begin{figure}
  \begin{center}
    \includegraphics[width=0.45\textwidth]{img/kantomap.png}
  \end{center}
  \caption{Target area for the Kanto region and its bins}
  \label{fig:kantomap}
\end{figure}

The forecast target is defined as a geographical region, a starting
date and an ending date. A forecast model will estimate the number
(and sometimes the magnitude) of earthquakes happening within this
target time/space.

A forecast is composed of \emph{bins}. Each bins represents a
geographical interval, measured in latitude and longitude, and
sometimes a magnitude interval. 

For example, in this paper we define the ``kanto'' region as as the
area covered by latitude N34.8 to N36.3, and longitude E138.8 to
E140.3. This area is divided into 2025 (45x45) bins of approximately
$5km^2$ (Figure~\ref{fig:kantomap}).

For each bin in the forecast we associate a (positive, integer) number
of expected earthquakes for that bin. A good forecast is that which
the number of estimated earthquakes in each bin corresponds to the
actual number of earthquakes that occurs in the target time period.

\subsection{Comparison Testing} 
%% TO REVIEW, 2nd paragraph can be reduced if needed.

The CSEP framework uses six different tests to compare earthquake
forecasts. These are divided into two groups: log-likelihood based
tests, and alarm based tests.

Log likelihood tests are based on a direct analysis of the similarity
between the forecast and the actual earthquake
catalog~\cite{Schorlemmer2007}. A Poissonian probability distribution
function is used to measure the likelihood between the forecast and
the data. Three statistical tests are defined using this metric: the
\emph{L-test}, which compares a forecast with the data, the
\emph{N-test}, which compares the total number of earthquakes forecast
with the total number of observed earthquakes, and the \emph{R-test},
which compares multiple forecasts at the same time. These tests
require that all the forecasts being compared have the same number and
size of bins.

Alarm based tests, on the other hand, are based on a treshold
analysis~\cite{Zechar2010}. All bins with forecast values above the
treshold are added to an ``alarm set''. The tests generate two values:
a miss rate $v$, the proportion of earthquakes in the data that
occurred in bins outside the alarm set; and the coverage rate $\tau$,
the proportion of the total bins in the model covered by the alarm
set. Three alarm based tests are defined: the \emph{Molcham Diagram}
draws a path of $v$ and $\tau$ based on varying values of the
treshold, the \emph{Area Skill Score (ASS)} summarizes a Molcham
Diagram into a single number. A Receiver Operating Characteristic
(ROC) can also be drawn using method of analysis. These testes do not
take into account the total number of earthquakes, either from the
forecast or the data.

\subsection{The Relative Intensity Algorithm}
%% TO REVIEW
%% TODO: maybe move this to experimental design.

The Relative Intensity (RI) algorithm is a commonly used benchmark for
earthquake prediction models~\cite{Nanjo2011}. In this paper, we use
it as goalpost to assess the suitability of evolutionary computation
for this problem.

The working assumption behind the RI is that larger earthquakes are
more likely to occur at locations of high seismicity in the
past. Accordingly, the RI algorithm will estimate the number of
earthquakes in a bin based on the number of earthquakes observed in
the past for that bin, plus an attenuation factor that takes into
account the seismicity of neighboring bins.

For more details on the implementation of the RI, please see the paper
by Nanjo~\cite{Nanjo2011}. In the experiments in this paper, we use
the following parameters: $b = 0.8$ and $s = 5$.

\section{Evolutionary Computation for Earthquake Risk Analysis}
%% TO REVIEW

Reports of the application of Evolutionary Computation and related
methods for the generation of earthquake forecasts are rather sparse.
One such approach is described by Zhang and
Wang~\cite{Zhang2012}. They use Genetic Algorithms to fine tune an
Artificial Neural Network (ANN), and use this system to produce a
forecast. Unfortunately that paper did not provide enough information
to reproduce the proposed GA+ANN system or their results. Zhou and
Zu~\cite{Feiyan2014} also recently proposed a combination of ANN and
EC, but their system only forecasts the magnitude parameter of
earthquakes.

On the other hand, there are quite a few works using Evolutionary
Computation methods for the estimation of parameter values in
seismological models. These models are used to describe and understand
particular characteristics of earthquakes or earthquake activity.

For example, there are many examples of using Evolutionary Computation
to estimate the peak ground acceleration of seismically active
areas~\cite{Kermani2009, Cabalar2009,Kerh2010}. Ramos~\cite{Ramos2011}
used Genetic Algorithms to decide the location of sensing stations in
a seismically active area in Mexico. Nicknam et. al~\cite{Nicknam2010}
and Kennett and Sambridge~\cite{Kennett1992} used evolutionary
computation to determine the Fault Model parameters (such as epicenter
location, strike, dip, etc) of a given earthquake.

\section{A Forecast Model Using Genetic Algorithms} 
%% TO REVIEW

To investigate the ability of Evolutionary Computation to generate
earthquake forecast models, we design and test a simple Genetic
Algorithm. Let us call this system the \emph{GAModel}.

An individual in GAModel will encode a forecast model as defined in
the CSEP framework~(see Section~\ref{CSEP_definition}). The population
will be trained on earthquake occurrence data for a fixed training
period. The best generated individual will be compared with a model
generated by the RI algorithm, and a skilless (random) model. This
comparison is based on earthquake occurrence data for a test period
immediately posterior to the training period.

By encoding the entire forecast model as one individual, we identified
two main concerns while designing GAModel: First, as the forecast
model contains a large number of bins, the genome of an individual
will be respectively large. Evolutionary operators and parameters must
be chosen carefully to guarantee convergence in a reasonable time, and
avoid local optima.

Secondly, the design of the fitness function deserves a lot of
attention, to avoid the risk of the system overfitting to the training
data.

\subsection{Genome Representation}
%% TO REVIEW
% IF space: add that mu can be adjusted if the intervals of test and
% train are different.
% IF space: might want to add a review of the bin definition

In GAModel, each individual represents an entire forecast model. The
genome is a real valued array $X$, where each element corresponds to
one bin in the desired model (the number of bins $n$ is defined by the
problem). Each element $x_i \in X$ can take a value from $[0,1)$. In
  the initial population, these values are sampled from a uniform
  distribution.

In the CSEP framework, a model is defined as a set of integer valued
expectations, corresponding to the number of predicted earthquakes for
each bin. To convert from the real valued chromossome to a integer
forecast, we use a modification of the Poisson deviates extraction
algorithm from~\cite{NumericalRecipes}~(Chapter 7.3.12).

\begin{algorithm}
  \caption{Obtain a poisson deviate from a $[0,1)$ value}
  \label{InversePoisson}
  \begin{algorithmic}
    \STATE Parameters $0 \leq x < 1, \mu \geq 0$
    \STATE $L \gets \exp{(-\mu)}, k \gets 0, prob \gets 1$
    \REPEAT 
    \STATE $\text{increment }k$
    \STATE $prob \gets prob*x$
    \UNTIL{$prob > L$}
    \RETURN $k$
  \end{algorithmic}
\end{algorithm}

In Algorithm~\ref{InversePoisson}, $x$ is the value taken from the
chromossome, and $\mu$ is the average number of earthquakes observed
across the entire training data. Note that in the original algorithm,
$k-1$ is returned. Because the log likelihood calculation used for
model comparison discards forecasts that estimate $0$ events in bins
where earthquakes are observed, we modify the original algorithm here
to make sure all bins estimate at least one event.

%% TODO: If We have space -- Explain that \mu is resized if the time
%% intervals are different between the training and the testing data
%% sets.

\subsection{Fitness Function}

The main challenge when applying an Evolutionary Computation method to
any new application domain is usually the definition of an appropriate
fitness function. Accordingly, the largest part of our effort in this
work was to define a good fitness function for GAModel.

\subsubsection{Simple Log Likelihood Fitness Function} %% DONE

Our first candidate was the log-likelihood between the forecast
generated by an individual and the observed earthquakes in the
training data, as described by Schorlemmer
et. al.~\cite{Schorlemmer2007}. In simple terms, the log likelihood is
a measure of how close a forecast is to a given data set.

Let $\Lambda = \{\lambda_1, \lambda_2, \dots, \lambda_n | \lambda_i
\in \mathbb{N}\}$ be a forecast with $n$ bins. In this definition,
$\lambda_i$ is the number of earthquakes that is forecast to happen in
bin $i$. To derive $\Lambda_X$ from an individual $X = \{x_1, x_2,
\dots, x_n | 0 \leq x_i < 1\}$, we calculate each $\lambda_i$ from
$x_i$ using Algorithm \ref{InversePoisson}.

Let $\Omega = \{\omega_1, \omega_2, \dots, \omega_n | \omega_i \in
\mathbb{N}\}$ be the observed earthquakes in the training data. The
log likelihood between an individual's forecast $\Lambda_X$ and the
observed data $\Omega$ can be calculated as:
\begin{equation}
  L(\Lambda_X|\Omega) = \sum_{i=0}^n {-\lambda_i +
    \omega_i*\ln(\lambda_i)-ln(\omega_i!)}
\end{equation}
There are two special cases that arise when any $\lambda_i = 0$. If
$\lambda_i = 0$ and $\omega_i = 0$, then the value of the sum for that
element is $1$. If $\omega_i > 0$, then $L(\Lambda_X|\Omega) =
-\infty$ and the forecast must be discarded. For more details on
this, see~\cite{Schorlemmer2007}.

In the Simple Log Likelihood fitness function, the value of
$L(\Lambda_X|\Omega)$ is taken as the fitness value of the individual.

Early testing with the Simple Log Likelihood function showed that
GAModel had a very strong tendency to overfit to the training
data. This is natural, since there are differences between the
seismocity of a larger period and a shorter one. 

To avoid this overfitting, we have developed two alternative fitness
functions.

\subsubsection{Simulated Log Likelihood Fitness Function}
%% REVIEW -- TODO: Possibly remove this subsection

In the Simulated Log Likelihood fitness function, we generate $k$
``simulations'' from the observed data $\Omega$, and compare an
individual's forecast with all the simulated data.

Let $\hat\Omega =
\{\hat\omega_1,\hat\omega_2,\dots,\hat\omega_3
|\hat\omega_i\in\mathbb{N}\}$ be a simulated data set generated from
$\Omega$. Each $\hat\omega_i$ is taken randomly from a Poissonian
distribution with mean $\omega_i$.

To calculate the Simulate Log Likelihood fitness, we calculate the Log
Likelihood of the forecast $\Lambda_X$ generated by an individual $X$
against each of the $k$ simulated data sets
($L(\Lambda_X|\hat\Omega_1), \dots, L(\Lambda_X|\hat\Omega_k)$). We
use the lowest of the $k$ log likelihood values as the fitness of $X$.

The idea behind this fitness function is that by training against a
number of similar, but slightly different training data sets at the
same time, we might be able to avoid overfitting the forecast to the
observed training data set.

\subsubsection{Time-slice Log Likelihood Fitness Function}
%% REVIEW

In the time-slice log likelihood fitness function we break up the
training data set into smaller \emph{slices}. These slices are based
on the chronology of the earthquakes contained in the training
catalog. The duration of each slice is the same as the duration of the
test catalog.

Let's consider an example where the target period for the forecast is
one year, from 1/1/2014 to 1/1/2015, and the training data is taken
from the 10 year period of 1/1/2004 to 1/1/2014. The time-slice log
likelihood fitness function will divide the training data into ten
1-year periods, from 2004 to 2005, 2005 to 2006, and so on. 

When an individual $X$ is evaluated, we calculate the log likelihood
of its forecast $\Lambda_X$ against each of the time slices
($\Omega_{2004}, \Omega_{2005}, \dots, \Omega_{2013}$). The lowest
value is used as the fitness of $X$.

The idea behind this fitness function is that the catalog data
available for training will normally span a period of time much longer
than the desired forecast. By breaking the training data into smaller
periods, we are trying to make the evolutionary algorithm learn any
time-repeating pattern that might exist in the data.

\subsection{Evolutionary Operators and Parameters}
%% REVIEW

GAModel uses a regular generational genetic algorithm. For selection,
we use Elitism and Tournament selection. 

For the crossover operator we use \emph{aNDX}, recently proposed by
Someya~\cite{Someya2013}. If the value of a chromossome after the
crossover falls outside the $[0,1)$ boundary, it is truncated to these
  limits. For the mutation operator, we sample entirely new values
  from $[0,1)$ for each mutated chromossome.

\begin{table}[!ht]
  \begin{center}
  \begin{tabular}{|l|r|}
    \hline
    Population Size & 500\\
    Generation Number & 100\\
    Elite Size & 1\\
    Tournament Size & 50\\
    Crossover Chance & 0.9\\
    aNDX parents & 4\\
    aNDX zeta & 0.5\\
    aNDX sigma & 1\\
    Mutation Chance (individual) & 0.8\\
    Mutation Chance (chromossome) & (genome size)$^{-1}$\\
    \hline    
  \end{tabular}
  \end{center}
  \caption{Parameters for GAModel}
  \label{GAParameters}
\end{table}

The parameters used for the evolutionary computation are described in
Table~\ref{GAParameters}. At this stage, we are not yet particularly
worried with convergence speed of the system. Because of this, not a
lot of effort was spent fine tuning these paremeter's values. Instead,
these values were choosen by trial and error on a data region not used
in the experiments of section~\ref{data}, until an acceptable
convergence time was found.

%% TODO: Overfitting comparison between the three fitness functions.

\section{Experiments}
%% TODO: Introduction to Experiments section
%% TODO: Goals of our experiments
%% Outline of the experiments

\subsection{Experimental Data}\label{data}

The data used in these experiments comes from the Japan Meteorological
Agency's (JMA) catalog. The catalog lists earthquakes recorded by the
sensing station network in Japan, along with the time, magnitude,
latitude, longitude and depth of the hypocenter.

The part of the catalog used in this work spans a period from 2000 to
2013, and has over 220.000 earthquakes recorded in the Japanese
archipelago. In order to better understand the predictive power of
GAModel, we divide the catalog into 5 areas, which are used in the
experiments below. The relative position and size of these areas can
be seen in Figure~\ref{areamap}.

\begin{enumerate}
  \item {\bf All Japan:}
  \item {\bf Sendai:}
  \item {\bf Kantou:}
  \item {\bf Kansai:}
  \item {\bf Touhoku:}
\end{enumerate}

%% TODO: Geographical Description of the Data (depth, number of bins, reasoning)
%% TODO: Figures of the locations we chose for the experiments

\subsection{Experimental Design} %% DONE

To test the ability of a Genetic Algorithm to generate an effective
earthquake risk model, we perform a forecast experiment. In this
experiment we compare three forecasts: a randomly generated forecast,
one generated by the RI algorithm, and a group of forecasts generated
by the proposed method.

We define a ``scenario'' as a region and a forecast period. In this
experiment, we will test 5 regions (all japan, pacific, kanto, tohoku,
kansai), and 7 one-year periods (from 2005 to 2012), for a total of 40
scenarios.

Each scenario has a corresponding training window, which consists of
the 5 years prior to the scenario's forecast time. This training
period is used by the RI algorithm and by the proposed method to
generate their respectives forecasts. For the sake of statistical
testing, we generate 20 forecasts using the GAModel. Unless noted
otherwise, all results reported are an average of these runs.

%% TODO: Remove ASS if we end up not using it.
We compare the forecasts in three different ways: Using the
log-likelihood between the forecast and the testing data, Using the
ASS value, and visually by comparing the forecast maps of some
significative scenarios.

\subsection{Results and Comparison of forecast models}

\begin{table*}[t]
  \begin{center}
  \begin{tabular}{|ll||c|c|c|c||c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Scenario} & \multicolumn{4}{|c|}{Log Likelihood} & \multicolumn{4}{|c|}{Area Skill Score}\\
     & & Random & RI & GA & p-value & Random & RI & GA & p-value\\
    \hline
    East Japan & 2005 & -2666.11  &-1049.82 &-868.87 (20) & 0.001 $>$ & 0.47 & 0.35 & 0.30 (0.02) & 1 \\
    & 2006 & -6596.76 & -2130.69  &-2303.78 (98) & 1 & 0.46 & 0.42 & 0.35 (0.01) & 1 \\
    & 2007 & -6714.36 & -1997.78  &-2065.4 (85) & 0.99 & 0.43 & 0.51 & 0.34 (0.02) & 1 \\
    & 2008 &-8784.64  & -6087.88  &-6097.63 (264) & 0.56 & 0.46 & 0.23 & 0.20 (0.03) & 1 \\
    & 2009 & -6435.07 & -2052.35  &-1964.72 (106) & 0.001 $>$ & 0.47 & 0.49 & 0.48 (0.02) & 0.99\\
    & 2010 &-6560.97  & -2572.97  &-2541.33 (121) & 0.12 & 0.46 & 0.41 & 0.31 (0.01) & 1 \\
    & 2011 & -31447.79& -47704.35 &-51485.77 (536) & 1 & 0.44 & 0.40 & 0.18 (0.01) & 1 \\
    & 2012 & -19068.86& -5177.55  &-6657.52 (478) & 1 & 0.50 & 0.77 & 0.71 (0.01) & 1 \\
    \hline
    Kanto & 2005 &-3716.86 & -2263.4&-2253.2 (16.5) & 0.006 & 0.38 & 0.24 & 0.24 (0.04) & 0.78\\
    & 2006 &-3884.85 & -2252.28     &-2234.72 (14) & 0.001 $>$ & 0.36 & 0.10 & 0.18 (0.01) & 0.001 $>$\\
    & 2007 &-3838.9 & -2113.84      &-2108.95 (11.1) & 0.03 & 0.36 & 0.15 & 0.19 (0.02) &  0.001 $>$\\
    & 2008 & -3914.54&-2110.79      &-2096.75 (11.8) & 0.001 $>$ & 0.39 & 0.16 & 0.22 (0.3) & 0.001 $>$\\
    & 2009 &-4211.28 &-2487.88      &-2482.88 (10.3) & 0.02 & 0.36 & 0.09 & 0.14 (0.01) & 0.001 $>$\\
    & 2010 & -4010.47&-2132.11      &-2099.13 (16.3) & 0.001 $>$ & 0.39 & 0.14 & 0.28 (0.03) & 0.001 $>$\\
    & 2011 &-17657.43 &-20083.09    &-19983.73 (144.4) & 0.003 & 0.35 & 0.07 & 0.08 (0.02) & 0.14\\
    & 2012 & -10863.99&-3225.39     &-4435.34 (248) & 1 & 0.48 & 0.80 & 0.77 (0.01) & 1 \\
    \hline
    Kansai & 2005 &-2219.06 &-1605 &-1631.96 (26) & 0.99 & 0.24 & 0 & 0.07 (0.02) & 0.001 $>$\\
    & 2006 & -2172.29&-1606 &-1631.19 (23.9) & 0.99 & 0.23 & 0 & 0.05 (0.01) & 0.001 $>$\\
    & 2007 &-2024.77 &-1615 &-1617.01 (2.3) & 0.99 & 0.22 & 0 & 0.03 (0.01) & 0.001 $>$\\
    & 2008 &-2038.16 &-1608 &-1610.41 (1.54) & 1 & 0.22 & 0 & 0.01 (0.01) & 0.003\\
    & 2009 &-2054.23 &-1618 &-1619.51 (2.58) & 0.99 & 0.21 & 0 & 0.02 (0.01) & 0.001 $>$\\
    & 2010 & -2054.63&-1613 &-1611.00 (2.27) & 0.001 $>$ & 0.27 & 0 & 0.07 (0.02) & 0.001 $>$\\
    & 2011 &-2059.89 &-1625 &-1625.12 (2.46) & 0.58 & 0.26 & 0 & 0.05 (0.03) & 0.001 $>$\\
    & 2012 &-2080.35 &-1601 &-1603.59 (2.92) & 0.99 & 0.22 & 0 & 0.04 (0.02) & 0.001 $>$\\
    \hline
    Touhoku & 2005 &-2552.61 &-1067.38 &-984.23 (84) & 0.001 $>$ & 0.58 & 0.58 & 0.62 (0.01) & 0.001 $>$\\
    & 2006 &-2613.1 &-1044.72 &-1073.03 (154) & 0.78 & 0.52 & 0.50 & 0.42 (0.03) & 1\\
    & 2007 & -2666.11&-1049.82 &-999.64 (83.6) & 0.007 & 0.51 & 0.51 & 0.41 (0.01) & 1\\
    & 2008 & -5124.54&-5007.49 &-4704.15 (131) & 0.001 $>$ & 0.36 & 0.05 & 0.18 & 0.001 $>$\\
    & 2009 & -2737.47&-1049.22 &-936.63 (60) & 0.001 $>$ & 0.54 & 0.67 & 0.70 (0.01) & 0.001 $>$\\
    & 2010 & -2714.68&-1045.03 &-1077.95 (136) & 0.85 & 0.53 & 0.66 & 0.57 & 1\\
    & 2011 & -3435.67&-2753.95 &-2963.31 (88) & 1 & 0.40 & 0.21 & 0.10 (0.01) & 1\\
    & 2012 &-3623.22 &-1326.52 &-1186.1 (45.3) & 0.001 $>$ & 0.47 & 0.62 & 0.70 (0.05) & 0.001 $>$\\
    \hline
  \end{tabular}
  \end{center}
  \caption{Results from the simulation experiments. The parenthesis
    after the GA values are the standard deviation of 20 runs. The two
    ``p-value'' columns report the significance of the mean-test for
    the alternate hypothesis: ``The GA average is higher than the RI
    result''.}
  \label{bigtable}
\end{table*}

%% TODO: Table with all the log likelihood results
%% Indicate whether the difference between RI and GA is statistically significant

%% TODO: Table with the ASS results
%% Indicate whether the difference between RI and GA is statistically significant

%% TODO: Choose some scenarios (maybe 3?) to visually compare

%% TODO: If there is not enough space, indicate that extra results
%% will be left as additional material

%% TODO: Justify the use of Molcham Diagrams and ASS

\section{Conclusion}

%% TODO: Conclusions and Future Work

-- We choose the parameters hapzardly - a more careful parameter
selection might improve the result (or at least the running time)


-- There is a large number of problems related to the development of
forecast models: Parameter optimization, model construction,
earthquake clustering, etc.

-- On the evolutionary side, we think GA or ANNs might be a good idea.

-- We encourage people to work on this with us.

\subsection{Future Work}




\section*{Acknowledgements}

TODO: Check with Bogdan if we need to provide Acknowledgements for the
JMA data.

% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{earthquake}

%\balancecolumns
\end{document}
